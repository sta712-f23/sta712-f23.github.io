\documentclass[11pt]{article}

\usepackage{authblk} % author list

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{bm}
\usepackage{hyperref}

% More generous page layout
\usepackage{geometry}

\usepackage{graphicx}
\usepackage{hyperref}


\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage[ttdefault=true]{AnonymousPro}
\usepackage{float}

% TODO Required for review.
\usepackage{setspace}
%\doublespacing

\title{STA 712: HW 2}

\author{}
\date{}

\begin{document}

\maketitle

\noindent \textbf{Due:} Friday, September 15, 12:00pm (noon) on Canvas\\

\noindent \textbf{Instructions:} Submit your work as a single PDF. Your document should be created using LaTeX; see the course website for a \href{https://sta712-f23.github.io/homework/hw_template.tex}{homework template file} and \href{https://sta712-f23.github.io/homework/latex_instructions/}{instructions} on getting started with LaTeX and Overleaf.

\section{Two views of AIC}

\subsection*{AIC and the KL divergence}

Let $Y$ denote the response variable of interest in a generalized linear model, and let $f_0(y)$ denote the \textit{true} probability function of the response (i.e., the true distribution that generated the data). For simplicity, assume here that $Y$ is continuous, so $f_0(y)$ is a density.\\

\noindent We consider a family of distributions $\{f_{\theta}(y) : \bm{\theta} \in \Theta \}$, parameterized by a vector $\bm{\theta} \in \mathbb{R}^p$. Our goal, when building a model, is to choose the parameters $\bm{\theta}$ which make $f_\theta(y)$ as close to the \textit{true} distribution $f_0(y)$ as possible.\\

\noindent One method of measuring how close $f_\theta(y)$ is to $f_0(y)$ is with the \textbf{Kullback-Leibler} (KL) divergence:
\begin{align}
K(f_\theta, f_0) = \int \log \left( \frac{f_0(y)}{f_\theta(y)} \right) f_0(y) dy.
\end{align}
We fit our model via maximum likelihood estimation, producing the maximum likelihood estimate $\widehat{\bm{\theta}}$ (which depends on the observed data). Then, $K(f_{\widehat{\theta}}, f_0)$ is one measure of how close our model is expected to fit a \textit{new} set of data. And one way of choosing a model would be to choose the model which does best at predicting new data, i.e. which minimizes the KL divergence.\\

\noindent In the following questions, you will explore some basics of KL divergence, and show that choosing a model to minimize AIC is equivalent to minimizing an estimate of the KL divergence.

\begin{enumerate}
\item Show that $K(f_\theta, f_0) = 0$ if $f_\theta = f_0$.

\item Using the fact that $log(x) \leq x - 1$ for all $x$, show that $K(f_\theta, f_0) \geq 0$ if $f_\theta \neq f_0$.

\item Conclude that, if we \textit{could} calculate KL divergence for our models, and the true model $f_0$ was among the models considered in the model selection procedure, then minimizing KL divergence would find the true model.

\item Explain why we can't actually calculate KL divergence when modeling real data.
\end{enumerate}

\noindent Since we can't calculate KL divergence, we to approximate it. Let $\bm{\theta}^*$ be the value of $\bm{\theta}$ which minimizes $K(f_\theta, f_0)$. A Taylor expansion gives
\begin{align}
K(f_{\widehat{\theta}}, f_0) \approx K(f_{\theta^*}, f_0) + \frac{1}{2} (\widehat{\bm{\theta}} - \bm{\theta}^*)^T \mathcal{I}(\bm{\theta}^*) (\widehat{\bm{\theta}} - \bm{\theta}^*),
\end{align}
where $\mathcal{I}(\bm{\theta}^*)$ is the information matrix at $\bm{\theta}^*$.

\begin{enumerate}
\item We know from STA 711 that, if the model is correct and the sample size is sufficiently large, then $(\widehat{\bm{\theta}} - \bm{\theta}^*)^T \mathcal{I}(\bm{\theta}^*) (\widehat{\bm{\theta}} - \bm{\theta}^*) \approx \chi^2_p$. Conclude that under these assumptions,
\begin{align}
\mathbb{E}[K(f_{\widehat{\theta}}, f_0)] \approx K(f_{\theta^*}, f_0) + p/2.
\end{align}
\end{enumerate}

\noindent So far, our approximation to $K(f_{\widehat{\theta}}, f_0)$ is $K(f_{\theta^*}, f_0) + p/2$ (we can already see a penalty term that involves the number of parameters!). However, $K(f_{\theta^*}, f_0)$ still depends on $f_0$. To estimate $K(f_{\theta^*}, f_0)$, let $\bm{Y} = [Y_1,...,Y_n]^T$ denote the observed data used to calculate $\widehat{\bm{\theta}}$, and consider the log-likelihood $\ell(\widehat{\bm{\theta}}) = -\sum_i \log f_{\widehat{\theta}}(Y_i)$.

\begin{enumerate}
\item Under the same assumptions as the previous question, $2(\ell(\widehat{\bm{\theta}}) - \ell(\bm{\theta}^*)) \approx \chi^2_p$. Use this to argue that 
\begin{align}
K(f_{\theta^*}, f_0) \approx \mathbb{E}[- \ell(\widehat{\bm{\theta}})] + p/2 + \int \log(f_0(y)) f_0(y) dy.
\end{align}

\item Combine the previous questions to argue that an estimate of $K(f_{\widehat{\theta}}, f_0)$ is
\begin{align}
\widehat{K(f_{\widehat{\theta}}, f_0)} = -\ell(\widehat{\bm{\theta}}) + p + \int \log(f_0(y)) f_0(y) dy.
\end{align}

\item Conclude that minimizing $\widehat{K(f_{\widehat{\theta}}, f_0)}$ is equivalent to minimizing
\begin{align*}
AIC = -2\ell(\widehat{\bm{\theta}}) + 2p.
\end{align*}
\end{enumerate}

\noindent This derivation isn't super rigorous, but it captures the main steps and the important intuition: AIC is a reasonable metric for model selection because minimizing AIC is equivalent to minimizing an estimate of the KL divergence.

\subsection*{AIC and LOOCV}

Another way of viewing AIC is as a computationally efficient approximation of leave-one-out cross-validation (LOOCV). Indeed, Stone (1977) showed that (under appropriate assumptions), \textit{AIC and LOOCV are asymptotically equivalent}. Stone's paper is quite short (indeed, it is actually published in the journals \textit{Notes, Queries, and Comments} rather than as a full-length research article), and this part of the assignment will guide you through reading the original paper.\\

\begin{quote}
Stone, M. (1977). \href{https://www.jstor.org/stable/2984877}{An asymptotic equivalence of choice of model by cross‚Äêvalidation and Akaike's criterion}. \textit{Journal of the Royal Statistical Society: Series B (Methodological)}, 39(1), 44-47.
\end{quote}

\noindent Begin by reading sections 1 -- 3 of the paper, which set up the background and problem the author is trying to solve. (Because this is a short research note, rather than a full paper, the structure of the article is a bit different than usual).

\begin{enumerate}
\item In this paper, what is the LOOCV metric that we want to minimize? That is, what value is calculated for each held-out observation?

\item The goal of the article is to show that model selection by AIC and LOOCV are asymptotically equivalent. In particular, which two quantities (give the equation numbers) does the author plan to show are equivalent?
\end{enumerate}

\noindent Now read section 4 (you may need to read it several times). For your first read-through, I suggest skimming over details like regularity conditions, and focusing on the general outline of the mathematical results. In particular:

\begin{enumerate}
\item What technique (which we have used many times in class!) does the author use to characterize the large-sample behavior of $A$?

\item What is the key assumption that makes $A$ asymptotically equivalent to AIC?
\end{enumerate}

\noindent Finally, read through section 4 again, keeping the outline and direction of the derivation in mind as you read. This time, pay more attention to the mathematical details and see if you can follow the steps. The key equations are (4.4), (4.5), and (4.6).

\begin{enumerate}
\item What asymptotic results are used to move from equation (4.4) to equation (4.5)?

\item How does the ``key assumption'' allow us to write equation (4.5) as equation (4.6)? In particular, what is required for $L_1 = -L_2$? (We have seen this equivalence before in STA 711!)
\end{enumerate}

\subsection*{Reflecting}

In this assignment, we have seen two justifications of AIC. First, using AIC is reasonable because minimizing AIC is the same as minimizing an estimate of the KL divergence. And minimizing an estimate of the KL divergence is reasonable because \textit{if} the true model is contained in our search, and \textit{if} we could minimize the actual KL divergence (not the estimate) then we would choose the true model. Second, minimizing AIC is reasonable because it is asymptotically equivalent to minimizing LOOCV error. As long as we think LOOCV error is a good method of assessing model performance, then AIC should be too.\\

\noindent Let's finish the assignment by comparing these two different perspectives of the AIC.

\begin{enumerate}
\item  What are the similarities between the two perspectives of AIC (KL divergence and LOOCV)? Think about:

\begin{itemize}
\item What KL divergence and LOOCV are trying to measure
\item Similar methods used in both perspectives
\end{itemize}

Provide an intuitive explanation for why the LOOCV error in the Stone (1977) paper (the log likelihood calculated with each training observation held out in turn) would be similar to the KL divergence (which involves expected log-likelihoods...)
\end{enumerate}

\end{document}