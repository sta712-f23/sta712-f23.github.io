{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Untitled\n",
        "format: html\n",
        "editor: visual\n",
        "editor_options:\n",
        "  chunk_output_type: console\n",
        "---"
      ],
      "id": "21922b82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "sba = pd.read_csv(\"https://sta712-f22.github.io/homework/sba_small.csv\")"
      ],
      "id": "34e2a468",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sba['Amount'] = np.log(sba['GrAppv'])\n",
        "sba['Default'] = sba['MIS_Status'] == 'CHGOFF'\n",
        "\n",
        "encoder = OneHotEncoder(drop = 'first', sparse = False)\n",
        "sba_encoded = encoder.fit_transform(sba[['UrbanRural', 'NewExist']])\n",
        "\n",
        "sba_encoded = pd.DataFrame(sba_encoded, columns = encoder.get_feature_names(['UrbanRural', 'NewExist']))\n",
        "sba_encoded['Amount'] = sba['Amount']\n",
        "\n",
        "# fitted values aren't exactly the same\n",
        "# newton-cg seems to give the closest results?\n",
        "m1 = LogisticRegression(penalty = 'none')\n",
        "m1.fit(sba_encoded, sba['Default'])\n",
        "\n",
        "(m1.intercept_, m1.coef_)\n",
        "\n",
        "# get the deviance\n",
        "2*log_loss(sba['Default'], m1.predict_proba(sba_encoded),\n",
        "normalize = False)"
      ],
      "id": "12b8ad89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's see that we get similar coefficient estimates and deviance in Python using scikit-learn:\n",
        "\n",
        "\n",
        "```{r}\n",
        "library(reticulate)\n",
        "library(tidyverse)\n",
        "sba <- py$sba\n",
        "sba <- sba %>%\n",
        "  mutate(Default = MIS_Status == \"CHGOFF\",\n",
        "         Amount = log(GrAppv),\n",
        "         UrbanRural = as.factor(UrbanRural),\n",
        "         NewExist = as.factor(NewExist))\n",
        "\n",
        "m2 <- glm(Default ~ UrbanRural + NewExist + Amount,\n",
        "          data = sba, family = binomial)\n",
        "summary(m2) # look, we get the same deviance!\n",
        "```\n",
        "\n",
        "\n",
        "Ok, but sklearn is a module that is really more useful for prediction than for inference. Instead, we could use the statsmodels module, which can be used more similarly to R.\n"
      ],
      "id": "0191099e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "sba_encoded['Intercept'] = 1\n",
        "m3 = sm.GLM(sba['Default'], sba_encoded, family=sm.families.Binomial())\n",
        "m3_results = m3.fit()\n",
        "\n",
        "# and now let's look at the results\n",
        "m3_results.summary()\n",
        "m3_results.deviance"
      ],
      "id": "8f0e5f38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Think about why the coefficients for the intercept, NewExist1, and NewExist2 might have such large standard errors here (and hence why the estimates are so different between R, scikit-learn, and statsmodels). It is because there are very very few \"unknown\" NewExist observations. This means that we have a really big issue with multicollinearity in the data! We should probably remove those \"unknown\" observations, or recode.\n",
        "\n",
        "Next, try to make a quantile residual plot in python! You'll need to write your own function to create quantile residuals.\n",
        "\n",
        "numpy.random.uniform to sample from a uniform scipy.stats.norm.ppf for the inverse cdf part"
      ],
      "id": "72877fac"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}