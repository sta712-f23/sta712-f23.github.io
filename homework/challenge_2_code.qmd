---
title: "STA 712 Challenge 2: Neural networks"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
library(reticulate)
```

```{r}
# simulate data for logistic regression
d <- 3
n <- 1000

x <- matrix(rnorm(d*n), nrow = n)
p <- exp(-0.5 + 0.5*x[,1] - 0.2*x[,2] + x[,3])/(
  1 + exp(-0.5 + 0.5*x[,1] - 0.2*x[,2] + x[,3])
)
y <- rbinom(n, 1, p)
```

```{r}
# fit logistic regression model
m1 <- glm(y ~ x, family = binomial)
```

```{r}
# Fitting the neural network in R
library(neuralnet)

train_data <- data.frame(x1 = x[,1],
                         x2 = x[,2],
                         x3 = x[,3],
                         y = y)
m2 <- neuralnet(y ~ x1 + x2 + x3, hidden = 0, data = train_data,
                err.fct = "ce",
                act.fct = "logistic",
                linear.output = FALSE)
# plot(m2)
```


```{python}
# fitting the neural network in Pytorch
import torch
from torch import nn

x = torch.tensor(r.x, dtype = torch.float32)
y = torch.tensor(r.y).float()

model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Sigmoid(),
    torch.nn.Flatten(0, 1)
)

loss_fn = torch.nn.BCELoss()

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use RMSprop; the optim package contains many other
# optimization algorithms. The first argument to the RMSprop constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)
for t in range(10000):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(x)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Backward pass: compute gradient of the loss with respect to model
    # parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its
    # parameters
    optimizer.step()
    
    # Before the backward pass, use the optimizer object to zero all of the
    # gradients for the variables it will update (which are the learnable
    # weights of the model). This is because by default, gradients are
    # accumulated in buffers( i.e, not overwritten) whenever .backward()
    # is called. Checkout docs of torch.autograd.backward for more details.
    optimizer.zero_grad()
```

```{python}
for param in model.parameters():
  print(param.data)
  
output = y_pred.tolist()
```


### Violating logistic regression assumptions


```{r}
# simulate data for logistic regression
d <- 3
n <- 1000

x <- matrix(rnorm(d*n), nrow = n)
p <- exp(-0.5 + 0.2*x[,1] + 0.1*x[,1]^2 - 0.01*x[,1]^3 - 
           0.2*sin(x[,2]) + log(x[,3]^2))/(
  1 + exp(-0.5 + 0.2*x[,1] + 0.1*x[,1]^2 - 0.01*x[,1]^3 - 
           0.2*sin(x[,2]) + log(x[,3]^2))
)
y <- rbinom(n, 1, p)
```

```{r}
# fit logistic regression model
m1 <- glm(y ~ x, family = binomial)
plot(m1, p)
```

```{r}
# Fitting the neural network in R
library(neuralnet)

train_data <- data.frame(x1 = x[,1],
                         x2 = x[,2],
                         x3 = x[,3],
                         y = y)
m2 <- neuralnet(y ~ x1 + x2 + x3, hidden = 3, data = train_data,
                err.fct = "ce",
                act.fct = "logistic",
                linear.output = FALSE)
# plot(m2)

plot(predict(m2, train_data)[,1], p)
```


```{python}
# fitting the neural network in Pytorch
import torch
from torch import nn

x = torch.tensor(r.x, dtype = torch.float32)
y = torch.tensor(r.y).float()

model = torch.nn.Sequential(
    torch.nn.Linear(3, 3),
    torch.nn.ReLU(),
    torch.nn.Linear(3, 3),
    torch.nn.ReLU(),
    torch.nn.Linear(3, 1),
    torch.nn.Sigmoid(),
    torch.nn.Flatten(0, 1)
)

loss_fn = torch.nn.BCELoss()

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use RMSprop; the optim package contains many other
# optimization algorithms. The first argument to the RMSprop constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)
for t in range(10000):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(x)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Backward pass: compute gradient of the loss with respect to model
    # parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its
    # parameters
    optimizer.step()
    
    # Before the backward pass, use the optimizer object to zero all of the
    # gradients for the variables it will update (which are the learnable
    # weights of the model). This is because by default, gradients are
    # accumulated in buffers( i.e, not overwritten) whenever .backward()
    # is called. Checkout docs of torch.autograd.backward for more details.
    optimizer.zero_grad()
    
output = y_pred.tolist()
```


```{r}
plot(py$output, p)
```


