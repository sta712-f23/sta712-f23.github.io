---
title: "Class activity solutions"
format: html
editor: source
author: "Ciaran Evans"
---

1. 

```{r}
n <- 100
nsim <- 1000
devs <- rep(NA, nsim)

for(i in 1:nsim){
  x <- rnorm(n)
  lambda <- exp(4 + 0.1*x)
  y <- rpois(n, lambda)
  m1 <- glm(y ~ x, family = poisson)
  devs[i] <- m1$deviance
}

qqplot(devs, rchisq(nsim, n-2))
abline(a=0, b=1)
```

As we can see, the quantiles for the simulated deviances fall very close to quantiles for a $\chi^2_{98}$ distribution, so the $\chi^2_{98}$ distribution is a good approximation for the distribution of residual deviance.

2. Now let's break the saddlepoint approximation. All we need to do is make the values of $Y_i$ smaller, so the approximation no longer holds. We can do this by making the $\lambda_i$s smaller:

```{r}
n <- 100
nsim <- 1000
devs <- rep(NA, nsim)

for(i in 1:nsim){
  x <- rnorm(n)
  lambda <- exp(0.1*x)
  y <- rpois(n, lambda)
  m1 <- glm(y ~ x, family = poisson)
  devs[i] <- m1$deviance
}

qqplot(devs, rchisq(nsim, n-2))
abline(a=0, b=1)
```

As we can see, the saddlepoint approximation is no longer very good!

3. 

```{r}
nsim <- 1000
n <- 100
pvals <- rep(NA, nsim)

for(i in 1:nsim){
  x <- rnorm(n)
  mu <- exp(4 + 0.1*x)
  y <- rnbinom(n, size=10, mu=mu)
  
  m1 <- glm(y ~ x, family = poisson)
  pvals[i] <- pchisq(m1$deviance, m1$df.residual, lower.tail=F)
}

mean(pvals < 0.05)
```

Here the power is 1, which makes sense because the NB distribution is not close to a Poisson distribution (*note*: as $r \to \infty$, the NB distribution approaches a Poisson. But, the values of $r$ which are "large" depend on the mean $\mu_i$).

4. I'll actually do the code for questions 4 and 5 together:

```{r}
rs <- c(5, 20, 50, 100, 200, 500, 1000)
n <- 100
nsim <- 1000

coverage <- rep(NA, length(rs))
power <- rep(NA, length(rs))

for(j in 1:length(rs)){
  res_coverage <- rep(NA, nsim)
  res_power <- rep(NA, nsim)
  for(i in 1:nsim){
    x <- rnorm(n)
    mu <- exp(4 + 0.1*x)
    y <- rnbinom(n, size=rs[j],mu=mu)
    
    m1 <- glm(y ~ x, family = poisson)
    
    upper <- summary(m1)$coefficients[2,1] + 
      1.96*summary(m1)$coefficients[2,2]
    lower <- summary(m1)$coefficients[2,1] - 
      1.96*summary(m1)$coefficients[2,2]
    
    res_coverage[i] <- upper > 0.1 && lower < 0.1
    res_power[i] <- pchisq(m1$deviance, m1$df.residual, lower.tail = F) < 0.05
  }
  
  coverage[j] <- mean(res_coverage)
  power[j] <- mean(res_power)
}


plot(rs, coverage)
plot(coverage, power)
```

As $r$ increases, power decreases (because the generating distribution becomes closer to a Poisson). Likewise, coverage increases with $r$. Plotting power against coverage, we can see that (at least in these simulations!) we have good power to detect violations of the Poisson assumption when it really matters: power is good when coverage is low. 

