---
title: "Class Activity"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

# Part I: Simulation study

In the first part of this class activity, you will conduct a simulation study to investigate the importance of the Poisson distribution assumption. 

* You will simulate data with

$$Y_i \sim NB(r, p_i)$$
$$\log(\mu_i) = \beta_0 + \beta_1 X_i$$
where $\mu_i = \mathbb{E}[Y_i] = \frac{rp_i}{1 - p_i}$, and $Var(Y_i) = \mu_i + \frac{\mu_i^2}{r}$. 

* Note that as $r \to \infty$, the $NB(r, p_i)$ distribution approaches a $Poisson(\mu_i)$ distribution. So, we can break the Poisson distribution assumption to different "degrees" by specifying different values of $r$. When $r$ is small, the Poisson distribution assumption is really wrong. When $r$ is large, the Poisson distribution assumption is not too bad!

* Importance of the Poisson distribution assumption will be assessed by coverage of (nominally) 95% confidence intervals for the true slope $\beta_1$. You will assess coverage for several different values of $r$.

## Starter code

The code below simulates $n = 100$ observations $X_i \sim N(0, 0.25)$ and $Y_i \sim Poisson(\lambda_i)$ with $\log(\lambda_i) = X_i$. It then calculates upper and lower bounds for a 95% confidence interval for the true slope ($\beta_1 = 1$), and checks whether the bounds contain $\beta_1$.

```{r, eval=F}
n <- 100

x <- rnorm(n, sd = 0.5)
y <- rpois(n, lambda = exp(x))

m1 <- glm(y ~ x, family = poisson)

upper <- summary(m1)$coefficients[2,1] + 
    1.96*summary(m1)$coefficients[2,2]
lower <- summary(m1)$coefficients[2,1] - 
    1.96*summary(m1)$coefficients[2,2]

upper > 1 && lower < 1
```

:::{.question}
#### Question 1

Using a `for` loop, repeat the starter code many times (say, `nsim = 1000`), and store the results. For what fraction of the simulations did the confidence interval actually contain $\beta_1$? (It should be very close to 95%!)
:::

## Breaking the Poisson distribution assumption

Now that we have verified that the confidence intervals have the desired coverage when the Poisson distribution assumption is satisfied, let's break the Poisson distribution assumption and see what happens!

The code below simulates $Y_i \sim NB(r, p_i)$ where $r = 10$ and $\mathbb{E}[Y_i] = \mu_i = \exp\{X_i\}$. It then calculates upper and lower bounds for a 95% confidence interval for the true slope ($\beta_1 = 1$), and checks whether the bounds contain $\beta_1$.

```{r, eval=F}
n <- 100
r <- 10

x <- rnorm(n, sd = 0.5)
y <- rnbinom(n, size = r, mu = exp(x))

m1 <- glm(y ~ x, family = poisson)

upper <- summary(m1)$coefficients[2,1] + 
    1.96*summary(m1)$coefficients[2,2]
lower <- summary(m1)$coefficients[2,1] - 
    1.96*summary(m1)$coefficients[2,2]

upper > 1 && lower < 1
```

:::{.question}
#### Question 2

Repeat the code above, with $Y_i \sim NB(r, p_i)$ and $r = 10$, many times. Now what fraction of the confidence intervals actually contain $\beta_1$?
:::

:::{.question}
#### Question 3

Repeat the process for different values of $r$, and plot confidence interval coverage as a function of $r$. How does coverage change as the Poisson distribution assumption becomes less reasonable (i.e. as $r$ decreases)?
:::


# Part II: Exploring quantile residual plots

In linear regression, residuals can be used to assess the shape, constant variance, and normality assumptions. In Poisson regression, we can use quantile residuals to assess both the shape and Poisson distribution assumptions. The purpose of this part of the class activity is to explore how quantile residual plots can diagnose violations of the Poisson regression assumptions.

## Questions

To begin, let's generate data from the model

$$X_i \sim N(0, 0.5)$$
$$\log(\lambda_i) = X_i$$
$$Y_i \sim Poisson(\lambda_i)$$
Note that the Poisson regression assumptions are satisfied for this model.

:::{.question}
#### Question 4

Run the code below to generate data from this model, fit the Poisson regression model in R, and create a quantile residual plot:

```{r, eval=F}
library(statmod)
library(tidyverse)

n <- 1000
x <- rnorm(n, sd = 0.5)
y1 <- rpois(n, lambda = exp(x))

m1 <- glm(y1 ~ x, family = poisson)

data.frame(x = x, resids = qresid(m1)) |>
  ggplot(aes(x = x, y = resids)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(x = "x", y = "Quantile residuals") 
```
:::


Now what happens if we change the distribution of $Y$? The negative binomial distribution is another count distribution, which is different from the Poisson (we will see much more of the negative binomial later!). Let's generate negative binomial data (violating the Poisson distribution assumption), and see how the quantile residual plot changes.

:::{.question}
#### Question 5

Run the code below to generate negative binomial data, fit a Poisson regression model, and make a quantile residual plot. What differences do you see in your plot compared to question 1?

```{r, eval=F}
n <- 1000
x <- rnorm(n, sd = 0.5)
y2 <- rnbinom(n, size=0.5, mu=exp(x))

m2 <- glm(y2 ~ x, family = poisson)

data.frame(x = x, resids = qresid(m2)) |>
  ggplot(aes(x = x, y = resids)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(x = "x", y = "Quantile residuals") 
```
:::

Finally, let's try violating the shape assumption with Poisson data. 

:::{.question}
#### Question 6

Modify your code from question 4 to violate the shape assumption, but not the Poisson distribution assumption. What differences do you see in your plot compared to question 4?
:::

:::{.question}
#### Question 7

Summarize how quantile residual plots can be used to identify issues with the Poisson regression assumptions.
:::

