---
title: "Class Activity, September 6"
output: 
  tufte::tufte_html:
    css: "lab.css"
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

# Model selection with the dengue data

In this class activity, we will use stepwise selection to select diagnostic variables to predict whether a patient has dengue. This uses the `stepAIC` function in the `MASS` package.

## Questions

1. Let's begin with forward selection, using AIC as a criterion. Use the code below to run forward selection. You can see that the procedure selects WBC, Age, PLT, BMI, Temperature, Vomiting, and DiseaseDay as predictors.

```{r eval=F}
library(MASS)

dengue <- read.csv("https://sta712-f22.github.io/homework/dengue.csv")

# specify the starting model (intercept-only)
m0 <- glm(Dengue ~ 1, data = dengue, family = binomial)

# forward selection using AIC
# Note we have to specify the largest model we want to consider
forward_aic <- stepAIC(m0, scope = ~ Sex + Age + DiseaseDay + 
                        Vomiting + Abdominal + Temperature + BMI + 
                        WBC + HCT + PLT,
                      direction = "forward",
                      trace = 0)

summary(forward_aic)
```

2. Now let's try forward selection with BIC instead. Do the variables selected change?

```{r eval=F}
# forward selection using BIC
# Note we have to specify the largest model we want to consider
# k = log(n) is used to specify the penalty for BIC instead of AIC
forward_bic <- stepAIC(m0, scope = ~ Sex + Age + DiseaseDay + 
                        Vomiting + Abdominal + Temperature + BMI + 
                        WBC + HCT + PLT,
                      direction = "forward",
                      trace = 0, k = log(nrow(dengue)))

summary(forward_bic)
```

3. In the dengue example, AIC and BIC actually gave us the same model. But often AIC and BIC give different models. Here is an example using data on risk factors associated with low infant birth weight (`bwt`), comparing backward selection with AIC and BIC; you can see that BIC selects a smaller model.

```{r eval=F}
example(birthwt) # (load the bwt data)
birthwt_glm <- glm(low ~ ., family = binomial, data = bwt)

birthwt_back_aic <- stepAIC(birthwt_glm, trace = 0)
birthwt_back_bic <- stepAIC(birthwt_glm, trace = 0, 
                            k = log(nrow(bwt)))

summary(birthwt_back_aic)
summary(birthwt_back_bic)
```

# Model selection and hypothesis testing

In the second part of this class activity, we will see what happens when we try to test hypotheses after performing variable selection. 

The code below generates $n = 500$ observations, with 100 different explanatory variables. The data is generated so there is *no relationship* between any of the explanatory variables and the response.

It then fits a model using all explanatory variables, and creates a histogram of the p-values for the hypothesis test $H_0: \beta_j = 0$ vs. $H_A: \beta_j \neq 0$ for each coefficient $\beta_j$. *By construction, $H_0$ is true for all these tests!*


```{r, eval=F}
library(MASS)

n <- 500
d <- 100
x <- matrix(rnorm(n*d),nrow=n)
y <- rbinom(n, 1, 0.5)
df <- data.frame(y=y,x)

m1 <- glm(y ~ ., data = df, family = binomial)
hist(summary(m1)$coefficients[,4], 
     xlab = "p-values", main = "Histogram of p-values")
```

## Questions

4. Run the above code several times. What do you notice about the distribution of the p-values? What do you conclude about the distribution of p-values when $H_0$ is true?

Now let's perform stepwise selection, and look only at the p-values of the variables selected:

```{r, eval=F}
m0 <- glm(y ~ 1, data = df, family = binomial)

forward_aic <- stepAIC(m0, scope = list(upper = m1),
                    direction = "forward",
                    trace = 0)

hist(summary(forward_aic)$coefficients[,4], 
     xlab = "p-values", main = "Histogram of p-values after model selection")

```

5. How has the distribution of p-values changed? What do you conclude about the distribution of p-values after model selection?

6. Explain why testing hypotheses *after* performing model selection is a **bad idea**.
